{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "Now that I've validated and cleaned up my dataset, I can analyze it. I'll quickly run everything I did in the [preparation and validation notebook](https://github.com/anushadatar/split-ticket/blob/master/Prepare-Dataset.ipynb) and then go on to actually explore split-ticket voting.\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xea in position 80: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4fe235bf169e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Load the dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/anush/OneDrive/Documents/anes_data/anes_timeseries_cdf_dta/anes_timeseries_cdf.dta\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0manes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Clean up missing data codes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xea in position 80: unexpected end of data"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "def sanitize_series(series):\n",
    "    \"\"\"\n",
    "    Turns missing data codes in Pandas series into NaN, and then returns \n",
    "    the pandas series.\n",
    "    \"\"\"\n",
    "    return series.map(lambda n: n if (n > 0) else np.nan)\n",
    "\n",
    "def sanitize_df(df):\n",
    "    \"\"\"\n",
    "    Turns missing data codes in Pandas series into NaN, and then returns \n",
    "    the pandas series.\n",
    "    \"\"\"\n",
    "    return df.applymap(lambda n: np.nan if (is_numeric_dtype(type(n)) and n < 0) else n)\n",
    "\n",
    "# Load the dataset.\n",
    "data_path = \"C:/Users/anush/OneDrive/Documents/anes_data/anes_timeseries_cdf_dta/anes_timeseries_cdf.dta\"\n",
    "anes = pd.read_csv(data_path)\n",
    "\n",
    "# Clean up missing data codes.\n",
    "anes = sanitize_df(anes)\n",
    "\n",
    "# Resample.\n",
    "n = len(anes)\n",
    "weights = anes['VCF0009z']\n",
    "sample = anes.sample(n, \n",
    "                     replace=True, \n",
    "                     weights=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a sanitized, resampled dataset, I need to address the first question my dataset can answer: how has the prevalence of split ticket voting changed over time?\n",
    "\n",
    "# Split Ticket Metrics\n",
    "The dataset provides a metric for split-ticketedness, but it only looks at the difference between presidential and house votes. This is limiting because it does not consider the Senate and by virtue of considering presidential choices each time it cuts the amount of years in half.\n",
    "\n",
    "In a general election year, the following outcomes are possible:\n",
    "\n",
    "Note D stands for Democrat, R for Republican, and I for Independent. There is no data for indpendent house or senate candidates. \n",
    "\n",
    "| President | Senate | House |\n",
    "|-----------|--------|-------|\n",
    "| R         | R      | R     |\n",
    "| R         | R      | D     |\n",
    "| R         | D      | R     |\n",
    "| R         | D      | D     |\n",
    "| D         | R      | R     |\n",
    "| D         | R      | D     |\n",
    "| D         | D      | R     |\n",
    "| D         | D      | D     |\n",
    "| I         | R      | D     |\n",
    "| I         | R      | D     |\n",
    "| I         | D      | R     |\n",
    "| I         | D      | D     |\n",
    "\n",
    "In a midterm election year, the following outcomes are possible:\n",
    "Note D stands for Democrat, R for Republican, and I for Independent. There is no data for indpendent house or senate candidates. \n",
    "\n",
    "| Senate | House |\n",
    "|--------|-------|\n",
    "| R      | R     |\n",
    "| R      | D     |\n",
    "| D      | R     |\n",
    "| D      | D     |\n",
    "\n",
    "\n",
    "## Metric Specification\n",
    "\n",
    "The metric I propose to use is a weighted sum of the split-ticketedness of each vote. \n",
    "\n",
    "The way I will measure that will be to assign each set of votes a score between 0 and 1. In doing so, I will ignore the independent candidates, which I can justify because while independent candidates generally align with the voters of a single party, there is no straightforward way to determine which independent candidate a survey participant voted for and how that aligns with the rest of their ballot.\n",
    "\n",
    "The updated metric will work as such:\n",
    "\n",
    "| President | Senate | House | Score |\n",
    "|-----------|--------|-------|-------|\n",
    "| R         | R      | R     | 1     |\n",
    "| R         | R      | D     | .66   |\n",
    "| R         | D      | R     | .33   |\n",
    "| R         | D      | D     | 0     | \n",
    "| D         | R      | R     | .66   |\n",
    "| D         | R      | D     | .33   |\n",
    "| D         | D      | R     | .33   |\n",
    "| D         | D      | D     | 0     |\n",
    "| I         | R      | D     | .5    |\n",
    "| I         | R      | D     | .5    |\n",
    "| I         | D      | R     | .5    |\n",
    "| I         | D      | D     | 1     |\n",
    "\n",
    "\n",
    "| Senate | House | Score |\n",
    "|--------|-------|-------|\n",
    "| R      | R     | 1     |\n",
    "| R      | D     | .5    |\n",
    "| D      | R     | .5    |\n",
    "| D      | D     | 1     |\n",
    "\n",
    "From there, I can assing a degree of splitness to each person as such.\n",
    "\n",
    "| Score | Description    |\n",
    "|-------|----------------|\n",
    "| 0     | Democrat       |    \n",
    "| 1     | Republican     |    \n",
    "| 0.33  | Lean Democrat  | \n",
    "| 0.66  | Lean Republican|   \n",
    "| 0.5   | 50/50 Split    | \n",
    "\n",
    "This way, any score greater than or equal to 3 is split.\n",
    "\n",
    "## Metric calculation\n",
    "To calculate this metric, I can map each vote to 0 or 1 (or NAN, if independent) and take the average of all of the votes and then remap it to an integer score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pres_vote = anes[\"VCF0706\"]\n",
    "house_vote = anes[\"VCF0707\"]\n",
    "senate_vote = anes[\"VCF0708\"]\n",
    "\n",
    "# Create new columns with re-mapped values.\n",
    "anes['pres_vote_ints'] = pres_vote.map({'2. Republican': 1, '1. Democrat': 0, '3. Other (incl. 3d/minor party candidates and write-ins)': np.nan, '7. Did not vote or voted but not for president (exc.1972)': np.nan})\n",
    "anes['house_vote_ints'] = house_vote.map({'2. Republican': 1, '1. Democrat': 0})\n",
    "anes['senate_vote_ints'] = senate_vote.map({'2. Republican': 1, '1. Democrat': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean() ignores missing values by default, so we can just take the average.\n",
    "means = anes[['pres_vote_ints', 'house_vote_ints', 'senate_vote_ints']].mean(axis=1).round(3)\n",
    "# Had to be very specific re: precision here.\n",
    "anes['split'] = means.map({0.000: 'Democratic Ballot', \n",
    "                         1.000: 'Republican Ballot', \n",
    "                         0.333: 'Democratic-Leaning Ballot', \n",
    "                         0.667: 'Republican-Leaning Ballot', \n",
    "                         0.500: '50/50 Split Ballot'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-Ticketedness Over Time\n",
    "\n",
    "Now that I computed the metric, I can view it over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = anes['VCF0004']\n",
    "split = anes['split']\n",
    "# Crosstabulate and normalize by year.\n",
    "xtab_norm = pd.crosstab(year, split, normalize='index')\n",
    "xtab_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I can find the count of voters with split votes and plot it by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Allen Downey's Political Alignment Case Study\n",
    "\n",
    "def decorate(**options):\n",
    "    \"\"\"Decorate the current axes.\n",
    "    Call decorate with keyword arguments like\n",
    "    decorate(title='Title',\n",
    "             xlabel='x',\n",
    "             ylabel='y')\n",
    "    The keyword arguments can be any of the axis properties\n",
    "    https://matplotlib.org/api/axes_api.html\n",
    "    \"\"\"\n",
    "    plt.gca().set(**options)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def make_lowess(series):\n",
    "    \"\"\"Use LOWESS to compute a smooth line.\n",
    "    \n",
    "    series: pd.Series\n",
    "    \n",
    "    returns: pd.Series\n",
    "    \"\"\"\n",
    "    y = series.values\n",
    "    x = series.index.values\n",
    "\n",
    "    smooth = lowess(y, x)\n",
    "    index, data = np.transpose(smooth)\n",
    "\n",
    "    return pd.Series(data, index=index)\n",
    "\n",
    "def plot_series_lowess(series, color):\n",
    "    \"\"\"Plots a series of data points and a smooth line.\n",
    "    \n",
    "    series: pd.Series\n",
    "    color: string or tuple\n",
    "    \"\"\"\n",
    "    series.plot(linewidth=0, marker='o', color=color, alpha=0.5)\n",
    "    smooth = make_lowess(series)\n",
    "    smooth.plot(label='_', color=color)\n",
    "\n",
    "\n",
    "def plot_columns_lowess(table, columns, colors):\n",
    "    \"\"\"Plot the columns in a DataFrame.\n",
    "    \n",
    "    table: DataFrame with a cross tabulation\n",
    "    columns: list of column names, in the desired order\n",
    "    colors: mapping from column names to colors\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        series = table[col]\n",
    "        plot_series_lowess(series, colors[col])\n",
    "\n",
    "def anchor_legend(x, y):\n",
    "    \"\"\"Place the upper left corner of the legend box.\n",
    "    \n",
    "    x: x coordinate\n",
    "    y: y coordinate\n",
    "    \"\"\"\n",
    "    plt.legend(bbox_to_anchor=(x, y), loc='upper left', ncol=1)\n",
    "        \n",
    "columns = ['Democratic Ballot', 'Republican Ballot', 'Democratic-Leaning Ballot', 'Republican-Leaning Ballot', '50/50 Split Ballot']\n",
    "\n",
    "muted = sns.color_palette('muted', 5)\n",
    "# Swap to align with the political spectrum colors.\n",
    "orange = muted[1]\n",
    "muted[1] = muted[3]\n",
    "muted[3] = orange\n",
    "\n",
    "color_map = dict(zip(columns, muted))\n",
    "muted[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns_lowess(xtab_norm, columns, color_map)\n",
    "decorate(xlabel='Year',\n",
    "         ylabel='Proportion in Group',\n",
    "         title='Ballot Consistency over Time')\n",
    "\n",
    "anchor_legend(1.02, 1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Party Identification vs. Voting Consistency\n",
    "\n",
    "Then, I'll look at the impact of political identification on voting consistency by looking at the split-ticketedness of different groups of people. Here, I want to look at the correlation between party identification and ticketedness of vote. \n",
    "\n",
    "I'll start with Democrats and Republican who identify directly with their parties. The ANES dataset asks for political affiliation and allows respondents to choose between Strong/Weak/Independent affiliation with the Democratic and Republican Party (and Independent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Democrats.\n",
    "strong_democrat_filter = (anes[\"VCF0301\"] == '1. Strong Democrat')\n",
    "weak_democrat_filter   = (anes[\"VCF0301\"] == '2. Weak Democrat')\n",
    "strong_democrat_split  = split[strong_democrat_filter]\n",
    "weak_democrat_split    = split[weak_democrat_filter]\n",
    "all_democrat_split = pd.concat([strong_democrat_split, weak_democrat_split])\n",
    "\n",
    "# Republicans.\n",
    "strong_republican_filter = (anes[\"VCF0301\"] == '7. Strong Republican')\n",
    "weak_republican_filter   = (anes[\"VCF0301\"] == '6. Weak Republican')\n",
    "strong_republican_split  = split[strong_republican_filter]\n",
    "weak_republican_split    = split[weak_republican_filter]\n",
    "all_republican_split = pd.concat([strong_republican_split, weak_republican_split])\n",
    "\n",
    "\n",
    "# Independents.\n",
    "independent_democrat_filter    = (anes[\"VCF0301\"] == '3. Independent - Democrat')\n",
    "independent_republican_filter  = (anes[\"VCF0301\"] == '5. Independent - Republican')\n",
    "independent_independent_filter = (anes['VCF0301'] == '4. Independent - Independent')\n",
    "independet_democrat_splits     = split[independent_democrat_filter]\n",
    "independent_republican_splits  = split[independent_republican_filter]\n",
    "independent_independent_splits = split[independent_independent_filter]\n",
    "all_independent_split = pd.concat([independet_democrat_splits, independent_republican_splits, independent_independent_splits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to be able to look at the correlation between voting records and sticking to expected party-line vote over the years.\n",
    "### Democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstabulate and normalize by year.\n",
    "\n",
    "xtab_norm = pd.crosstab(year, all_democrat_split, normalize='index')\n",
    "xtab_norm\n",
    "\n",
    "plot_columns_lowess(xtab_norm, columns, color_map)\n",
    "decorate(xlabel='Year',\n",
    "         ylabel='Proportion',\n",
    "         title='Ballot Consistency Among Democrats over Time')\n",
    "\n",
    "anchor_legend(1.02, 1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstabulate and normalize by year.\n",
    "xtab_norm = pd.crosstab(year, all_republican_split, normalize='index')\n",
    "xtab_norm\n",
    "\n",
    "plot_columns_lowess(xtab_norm, columns, color_map)\n",
    "decorate(xlabel='Year',\n",
    "         ylabel='Proportion',\n",
    "         title='Ballot Consistency Among Republicans over Time')\n",
    "\n",
    "anchor_legend(1.02, 1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab_norm = pd.crosstab(year, all_independent_split, normalize='index')\n",
    "xtab_norm\n",
    "\n",
    "plot_columns_lowess(xtab_norm, columns, color_map)\n",
    "decorate(xlabel='Year',\n",
    "         ylabel='Proportion',\n",
    "         title='Ballot Consistency Among Independents over Time')\n",
    "\n",
    "anchor_legend(1.02, 1.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
